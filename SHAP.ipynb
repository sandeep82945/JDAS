{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5 GPU(s) available.\n",
      "We will use the GPU: A100-PCIE-40GB\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'shap' has no attribute 'maskers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1203600/3024305102.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'MOT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CLA'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SOU'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'SUB'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MEA'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ORI'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'REP'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hardik/COLING_2022/shap.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"this work studies the predictive uncertainty issue of deep learning models . in particular  this work focuses on the distributional uncertainty which is caused by distributional mismatch between training and test examples . the proposed method is developed based on the existing work called dirichlet prior network  dpn  . it aims to address the issue of dpn that its loss function is complicated and makes the optimization difficult . instead  this paper proposes a new loss function for dpn  which consists of the commonly used crossentropy loss term and a regularization term . two loss functions are respectively defined over indomain training examples and outofdistribution  ood  training examples . the final objective function is a weighted combination of the two loss functions . experimental study is conducted on one synthetic dataset and two image datasets  cifar10 and cifar100  to demonstrate the properties of the proposed method and compare its performance with the relevant ones in the literature . the issue researched in this work is of significance because understanding the predictive uncertainty of a deep learning model has its both theoretical and practical value . the motivation [DEPT] research issues and the proposed method are overall clearly presented . the current recommendation is weak reject because the experimental study is not convincing or comprehensive enough . 1 .although the goal of this work is to deal with the inefficiency issue of the objective function of existing dpn with the newly proposed one  this experimental study does not seem to conduct sufficient experiments to demonstrate the advantages  say  in terms of training efficiency  the capability in making the network scalable for more challenging dataset  of the proposed objective function over the existing one  2 . table 1 compares the proposed method with odin . however  as indicated in this work  odin is trained with indomain examples only . is this comparison fair  actually  odin s setting seems to be more practical and more challenging than the setting used by the propose methods . 3 .the evaluation criteria shall be better explained at the beginning of the experiment  especially how they can be collectively used to verify that the proposed method can better distinguish distributional uncertainty from other uncertainty types . 4 .in addition  the experimental study can be clearer on the training and test splits . [DEPT] how many samples from cifar10 and cifar100 are used for training and test purpose  respectively  also  since training examples are from cifar10 and cifar100 and the test examples are also from these two datasets  does this contradict with the motivation of distributional mismatch between training and test examples mentioned in the abstract  5 .the experimental study can have more comparison on challenging datasets with more classes since it is indicated that dpn has difficulty in dealing with a large number of classes . minor  1 . please define the hattheta in eq .also  is the dirac delta estimation a good enough approximation here  2 .the lambda  out   lambda  in  in eq .  11  needs to be better explained . in particular  are the first terms in eq .  10  and eq .  11  comparable in terms of magnitude  otherwise  lambda  out   lambda  in  may not make sense . 3 .the novelty and significance of finetuning the proposed model with noisy ood training images can be better justified .\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m \u001b[0mmasker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaskers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"transformers tokenizer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0mexplainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_prediction_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0mshap_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'shap' has no attribute 'maskers'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "import shap\n",
    "\n",
    "labels = classes = ['MOT', 'CLA', 'SOU','SUB', 'MEA', 'ORI', 'REP']\n",
    "\n",
    "\n",
    "#If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda:4\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "  print('No GPU available, using the CPU instead.')\n",
    "  device = torch.device(\"cpu\")\n",
    "\n",
    "#torch.cuda.set_device(0)\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import pickle\n",
    "import torch\n",
    "from tqdm.autonotebook import tqdm\n",
    "from transformers import AutoTokenizer,AutoModel\n",
    "\n",
    "model_name='bert-base-uncased'\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import pickle\n",
    "import torch\n",
    "from tqdm.autonotebook import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import nltk\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():    \n",
    "    device = torch.device(\"cuda:4\")\n",
    "\n",
    "else:\n",
    "   device = torch.device(\"cpu\")\n",
    "\n",
    "torch.cuda.set_device(2)\n",
    "\n",
    "\n",
    "class BertModel(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "\n",
    "        super(BertModel, self).__init__()\n",
    "\n",
    "        self.in_features = in_features   #768\n",
    "        self.out_features = out_features    #7\n",
    "\n",
    "        self.flatten=nn.Flatten()\n",
    "        self.lstm_1 = nn.LSTM(in_features, 512//2, batch_first=True, bidirectional=True) #bidirectional=True\n",
    "\n",
    "        self.linear1=nn.Linear(512*7,512*2)\n",
    "        self.linear2=nn.Linear(512*2,256)\n",
    "        self.linear3=nn.Linear(256,64)\n",
    "\n",
    "        self.last_dense = nn.Linear(64, self.out_features)\n",
    "        self.dropout1=nn.Dropout(p=0.4)\n",
    "        self.dropout2=nn.Dropout(p=0.2)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "\n",
    "        self.category=nn.Linear(512,out_features)\n",
    "        \n",
    "        #SENTIMENT PART\n",
    "\n",
    "        self.linear_sen1=nn.Linear(512,256)\n",
    "        self.linear_sen2=nn.Linear(256,32)\n",
    "        self.linear_sen3=nn.Linear(32,2)\n",
    "        \n",
    " \n",
    "        #DECSION PART OF THE MODEL       \n",
    "        hidden_dimension=512\n",
    "    \n",
    "        self.linear_des1=nn.Linear(hidden_dimension*3,256)\n",
    "        self.linear_des2=nn.Linear(256,64)\n",
    "        self.linear_des3=nn.Linear(64,8)\n",
    "        self.linear_des4=nn.Linear(8,1)\n",
    "\n",
    "        imp_score= torch.rand( 1,7,requires_grad=True)  #(512,1)\n",
    "        nn.init.xavier_normal_(imp_score)\n",
    "        \n",
    "        self.imp_score=imp_score.to(device)\n",
    "        \n",
    "    def common(self, review):\n",
    "\n",
    "        s_e=review                                    #(4,40,768)\n",
    "\n",
    "        h0 = torch.zeros(2, s_e.size(0), 512 // 2)\n",
    "        c0 = torch.zeros(2, s_e.size(0), 512 // 2)\n",
    "        h0, c0 = h0.to(device), c0.to(device)\n",
    "        s_e, (hn, cn) = self.lstm_1(s_e, (h0, c0))    #(4,40,512)\n",
    "\n",
    "        l = self.relu(self.linear_sen1(s_e))\n",
    "        l = self.dropout1(l)\n",
    "        l = self.relu(self.linear_sen2(l))\n",
    "        l = self.dropout1(l)\n",
    "        l = self.relu(self.linear_sen3(l))     #(4,40,2)\n",
    "\n",
    "\n",
    "        comp=self.category(s_e)         #(4,40,7)\n",
    "        comp = comp.permute(0,2,1)      #(4,7,40)\n",
    "        \n",
    "        \n",
    "        wts = F.softmax(comp, dim=2) #(4,7,40)\n",
    "        e=torch.bmm(wts,s_e)       #(4,7,512)\n",
    "\n",
    "        out_sen=torch.matmul(wts,l)  #(4,7,2)\n",
    "\n",
    "        \n",
    "        l = torch.reshape(e, (s_e.size(0), -1)) #(4,7*512)\n",
    "        \n",
    "        l = self.relu(self.linear1(l))\n",
    "        l = self.dropout1(l)\n",
    "        l = self.relu(self.linear2(l))\n",
    "        l = self.dropout1(l)\n",
    "        l = self.relu(self.linear3(l))\n",
    "\n",
    "        model_output = self.sigmoid(self.last_dense(l))\n",
    "        model_output_sen =self.sigmoid(out_sen)\n",
    "\n",
    "        return model_output, e, wts,model_output_sen\n",
    "\n",
    "    \n",
    "    def forward(self, review1,review2,review3):\n",
    "\n",
    "        aspect_output1,dse1,imp_s1,out_sen1=self.common(review1)\n",
    "        aspect_output2,dse2,imp_s2,out_sen2=self.common(review2)\n",
    "        aspect_output3,dse3,imp_s3,out_sen3=self.common(review3)\n",
    "\n",
    "        i_p_aspect = F.softmax(self.imp_score,dim=1).to(device)  #(1,7)\n",
    "        i_p_aspect= i_p_aspect.unsqueeze(dim=1) #(1,1,7)\n",
    "        \n",
    "        dse1=torch.matmul(i_p_aspect,dse1).squeeze(dim=1)  #(n_p,512)\n",
    "        dse2=torch.matmul(i_p_aspect,dse2).squeeze(dim=1)  #(n_p,512)\n",
    "        dse3=torch.matmul(i_p_aspect,dse3).squeeze(dim=1)  #(n_p,512)\n",
    "\n",
    "        final_des_embed=torch.cat((dse1,dse2,dse3),dim=1)  #(n_p,512*3)\n",
    "\n",
    "        x=self.relu(self.linear_des1(final_des_embed))  #(n_p,256)\n",
    "        x=self.relu(self.linear_des2(x))  #(n_p,64)\n",
    "        x=self.relu(self.linear_des3(x)) # (n_p,8)\n",
    "        x=self.relu(self.linear_des4(x)) # (n_p,1)\n",
    "\n",
    "        output=self.sigmoid(x)\n",
    "        \n",
    "        return aspect_output1,aspect_output2,aspect_output3,imp_s1,imp_s2,imp_s3,output,out_sen1,out_sen2,out_sen3\n",
    "        \n",
    "        \n",
    "text_model = BertModel(768,7)\n",
    "text_model.to(device)\n",
    "text_model.load_state_dict(torch.load('/home/sandeep_2121cs29/hardik/COLING_2022/ckpt/bert_coling_multi.pt',map_location=device))\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('stsb-roberta-base')\n",
    "model=model.to(device)\n",
    "\n",
    "def create_embeddings(r):\n",
    "    sentences=nltk.sent_tokenize(r)\n",
    "    max_length=40\n",
    "\n",
    "    if len(sentences)<=max_length:\n",
    "        sentences=sentences+[\"\"]*(max_length-len(sentences))\n",
    "\n",
    "    else:\n",
    "        sentences=sentences[0:max_length]\n",
    "\n",
    "    encoded=model.encode(sentences, show_progress_bar=False)\n",
    "    \n",
    "    return encoded\n",
    "\n",
    "\n",
    "def f(x):\n",
    "   \n",
    "  r=x.split('[DEPT]')\n",
    "  x1=r[0]\n",
    "  x2=r[1]\n",
    "  x3=r[2]\n",
    "\n",
    "  e1=create_embeddings(x1)\n",
    "  e2=create_embeddings(x2)\n",
    "  e3=create_embeddings(x3)\n",
    "\n",
    "  #   inputs = tokenizer(x,add_special_tokens=False,\n",
    "  #     return_token_type_ids=True,\n",
    "  #     return_length = True,\n",
    "  #     truncation=False)\n",
    "  #   # #inputs = torch.tensor([tokenizer.encode(v, padding='max_length', max_length=128, truncation=True) for v in x]).cuda()\n",
    "            \n",
    "  #   ids = torch.tensor(inputs['input_ids']).to(device,dtype = torch.long)\n",
    "  #   mask = torch.tensor(inputs['attention_mask']).to(device,dtype = torch.long)\n",
    "  #   token_type_ids = torch.tensor(inputs['token_type_ids']).to(device,dtype = torch.long)\n",
    "  #t1=(ids,mask,token_type_ids)\n",
    "  \n",
    "  out1,out2,out3, attn_t1,attn_t2,attn_t3 ,d,out_sen1,out_sen2,out_sen3 = text_model(e1,e2,e3)\n",
    "\n",
    "  ret = d.cpu().detach().numpy()\n",
    "\n",
    "  print(ret)\n",
    "  return ret\n",
    "\n",
    "\n",
    "def model_prediction_gpu(x):\n",
    "    tv = torch.tensor([tokenizer.encode(v, padding='max_length', \n",
    "                                        max_length=512, truncation=True) for v in x]).to(device,dtype = torch.long)\n",
    "    attention_mask = (tv!=0).type(torch.int64).to(device,dtype = torch.long)\n",
    "    token_type_ids = torch.zeros_like(attention_mask).to(device,dtype = torch.long)\n",
    "    t1=(tv,attention_mask,token_type_ids)\n",
    "    out_test, attn_T,attn_T_S,out_test_senti = text_model(t1,'last')\n",
    "    val = torch.logit(out_test).detach().cpu().numpy()\n",
    "    return val\n",
    "\n",
    "\n",
    "method = \"custom tokenizer\"\n",
    "\n",
    "# build an explainer by passing a transformers tokenizer\n",
    "if method == \"transformers tokenizer\":\n",
    "    explainer = shap.Explainer(f, tokenizer, output_names=labels)\n",
    "\n",
    "# build an explainer by explicitly creating a masker\n",
    "elif method == \"default masker\":\n",
    "    masker = shap.maskers.Text(r\"\\W\") # this will create a basic whitespace tokenizer\n",
    "    explainer = shap.Explainer(f, masker, output_names=labels)\n",
    "\n",
    "# build a fully custom tokenizer\n",
    "elif method == \"custom tokenizer\":\n",
    "    import re\n",
    "\n",
    "    def custom_tokenizer(s, return_offsets_mapping=True):\n",
    "        \"\"\" Custom tokenizers conform to a subset of the transformers API.\n",
    "        \"\"\"\n",
    "        pos = 0\n",
    "        offset_ranges = []\n",
    "        input_ids = []\n",
    "        for m in re.finditer(r\"\\W\", s):\n",
    "            start, end = m.span(0)\n",
    "            offset_ranges.append((pos, start))\n",
    "            input_ids.append(s[pos:start])\n",
    "            pos = end\n",
    "        if pos != len(s):\n",
    "            offset_ranges.append((pos, len(s)))\n",
    "            input_ids.append(s[pos:])\n",
    "        out = {}\n",
    "        out[\"input_ids\"] = input_ids\n",
    "        if return_offsets_mapping:\n",
    "            out[\"offset_mapping\"] = offset_ranges\n",
    "        return out\n",
    "\n",
    "\n",
    "x = [\"this work studies the predictive uncertainty issue of deep learning models . in particular  this work focuses on the distributional uncertainty which is caused by distributional mismatch between training and test examples . the proposed method is developed based on the existing work called dirichlet prior network  dpn  . it aims to address the issue of dpn that its loss function is complicated and makes the optimization difficult . instead  this paper proposes a new loss function for dpn  which consists of the commonly used crossentropy loss term and a regularization term . two loss functions are respectively defined over indomain training examples and outofdistribution  ood  training examples . the final objective function is a weighted combination of the two loss functions . experimental study is conducted on one synthetic dataset and two image datasets  cifar10 and cifar100  to demonstrate the properties of the proposed method and compare its performance with the relevant ones in the literature . the issue researched in this work is of significance because understanding the predictive uncertainty of a deep learning model has its both theoretical and practical value . the motivation [DEPT] research issues and the proposed method are overall clearly presented . the current recommendation is weak reject because the experimental study is not convincing or comprehensive enough . 1 .although the goal of this work is to deal with the inefficiency issue of the objective function of existing dpn with the newly proposed one  this experimental study does not seem to conduct sufficient experiments to demonstrate the advantages  say  in terms of training efficiency  the capability in making the network scalable for more challenging dataset  of the proposed objective function over the existing one  2 . table 1 compares the proposed method with odin . however  as indicated in this work  odin is trained with indomain examples only . is this comparison fair  actually  odin s setting seems to be more practical and more challenging than the setting used by the propose methods . 3 .the evaluation criteria shall be better explained at the beginning of the experiment  especially how they can be collectively used to verify that the proposed method can better distinguish distributional uncertainty from other uncertainty types . 4 .in addition  the experimental study can be clearer on the training and test splits . [DEPT] how many samples from cifar10 and cifar100 are used for training and test purpose  respectively  also  since training examples are from cifar10 and cifar100 and the test examples are also from these two datasets  does this contradict with the motivation of distributional mismatch between training and test examples mentioned in the abstract  5 .the experimental study can have more comparison on challenging datasets with more classes since it is indicated that dpn has difficulty in dealing with a large number of classes . minor  1 . please define the hattheta in eq .also  is the dirac delta estimation a good enough approximation here  2 .the lambda  out   lambda  in  in eq .  11  needs to be better explained . in particular  are the first terms in eq .  10  and eq .  11  comparable in terms of magnitude  otherwise  lambda  out   lambda  in  may not make sense . 3 .the novelty and significance of finetuning the proposed model with noisy ood training images can be better justified .\"]\n",
    "\n",
    "masker = shap.maskers.Text(\"transformers tokenizer\")\n",
    "explainer = shap.Explainer(model_prediction_gpu, tokenizer, output_names=labels)\n",
    "shap_values = explainer(x)\n",
    "\n",
    "shap.plots.text(shap_values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: shap in /home/sandeep_2121cs29/anaconda3/envs/bart3/lib/python3.8/site-packages (0.40.0)\n",
      "Requirement already satisfied: tqdm>4.25.0 in /home/sandeep_2121cs29/anaconda3/envs/bart3/lib/python3.8/site-packages (from shap) (4.62.3)\n",
      "Requirement already satisfied: cloudpickle in /home/sandeep_2121cs29/anaconda3/envs/bart3/lib/python3.8/site-packages (from shap) (2.0.0)\n",
      "Requirement already satisfied: numpy in /home/sandeep_2121cs29/anaconda3/envs/bart3/lib/python3.8/site-packages (from shap) (1.21.6)\n",
      "Requirement already satisfied: slicer==0.0.7 in /home/sandeep_2121cs29/anaconda3/envs/bart3/lib/python3.8/site-packages (from shap) (0.0.7)\n",
      "Requirement already satisfied: scikit-learn in /home/sandeep_2121cs29/anaconda3/envs/bart3/lib/python3.8/site-packages (from shap) (1.0.2)\n",
      "Requirement already satisfied: pandas in /home/sandeep_2121cs29/anaconda3/envs/bart3/lib/python3.8/site-packages (from shap) (1.4.1)\n",
      "Requirement already satisfied: packaging>20.9 in /home/sandeep_2121cs29/anaconda3/envs/bart3/lib/python3.8/site-packages (from shap) (21.3)\n",
      "Requirement already satisfied: numba in /home/sandeep_2121cs29/anaconda3/envs/bart3/lib/python3.8/site-packages (from shap) (0.55.1)\n",
      "Requirement already satisfied: scipy in /home/sandeep_2121cs29/anaconda3/envs/bart3/lib/python3.8/site-packages (from shap) (1.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/sandeep_2121cs29/anaconda3/envs/bart3/lib/python3.8/site-packages (from packaging>20.9->shap) (3.0.7)\n",
      "Requirement already satisfied: setuptools in /home/sandeep_2121cs29/.local/lib/python3.8/site-packages (from numba->shap) (46.1.3)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /home/sandeep_2121cs29/anaconda3/envs/bart3/lib/python3.8/site-packages (from numba->shap) (0.38.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/sandeep_2121cs29/anaconda3/envs/bart3/lib/python3.8/site-packages (from pandas->shap) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/sandeep_2121cs29/anaconda3/envs/bart3/lib/python3.8/site-packages (from pandas->shap) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/sandeep_2121cs29/anaconda3/envs/bart3/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->shap) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/sandeep_2121cs29/anaconda3/envs/bart3/lib/python3.8/site-packages (from scikit-learn->shap) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/sandeep_2121cs29/anaconda3/envs/bart3/lib/python3.8/site-packages (from scikit-learn->shap) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/sandeep_2121cs29/anaconda3/envs/bart3/lib/python3.8/site-packages (4.19.0.dev0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/sandeep_2121cs29/anaconda3/envs/bart3/lib/python3.8/site-packages (from transformers) (0.4.0)\n",
      "Requirement already satisfied: filelock in /home/sandeep_2121cs29/anaconda3/envs/bart3/lib/python3.8/site-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/sandeep_2121cs29/anaconda3/envs/bart3/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /home/sandeep_2121cs29/anaconda3/envs/bart3/lib/python3.8/site-packages (from transformers) (0.11.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/sandeep_2121cs29/anaconda3/envs/bart3/lib/python3.8/site-packages (from transformers) (2022.1.18)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/sandeep_2121cs29/anaconda3/envs/bart3/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: requests in /home/sandeep_2121cs29/anaconda3/envs/bart3/lib/python3.8/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/sandeep_2121cs29/anaconda3/envs/bart3/lib/python3.8/site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/sandeep_2121cs29/anaconda3/envs/bart3/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/sandeep_2121cs29/anaconda3/envs/bart3/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/sandeep_2121cs29/anaconda3/envs/bart3/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/sandeep_2121cs29/anaconda3/envs/bart3/lib/python3.8/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sandeep_2121cs29/anaconda3/envs/bart3/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/sandeep_2121cs29/anaconda3/envs/bart3/lib/python3.8/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/sandeep_2121cs29/anaconda3/envs/bart3/lib/python3.8/site-packages (from requests->transformers) (1.26.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.\r\n",
      "To initialize your shell, run\r\n",
      "\r\n",
      "    $ conda init <SHELL_NAME>\r\n",
      "\r\n",
      "Currently supported shells are:\r\n",
      "  - bash\r\n",
      "  - fish\r\n",
      "  - tcsh\r\n",
      "  - xonsh\r\n",
      "  - zsh\r\n",
      "  - powershell\r\n",
      "\r\n",
      "See 'conda init --help' for more information and options.\r\n",
      "\r\n",
      "IMPORTANT: You may need to close and restart your shell after running 'conda init'.\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!conda activate hardik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5 GPU(s) available.\n",
      "We will use the GPU: A100-PCIE-40GB\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'shap' has no attribute 'maskers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1203600/3180454220.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'MOT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'CLA'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SOU'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'SUB'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MEA'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ORI'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'REP'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/hardik/COLING_2022/shap.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"this work studies the predictive uncertainty issue of deep learning models . in particular  this work focuses on the distributional uncertainty which is caused by distributional mismatch between training and test examples . the proposed method is developed based on the existing work called dirichlet prior network  dpn  . it aims to address the issue of dpn that its loss function is complicated and makes the optimization difficult . instead  this paper proposes a new loss function for dpn  which consists of the commonly used crossentropy loss term and a regularization term . two loss functions are respectively defined over indomain training examples and outofdistribution  ood  training examples . the final objective function is a weighted combination of the two loss functions . experimental study is conducted on one synthetic dataset and two image datasets  cifar10 and cifar100  to demonstrate the properties of the proposed method and compare its performance with the relevant ones in the literature . the issue researched in this work is of significance because understanding the predictive uncertainty of a deep learning model has its both theoretical and practical value . the motivation [DEPT] research issues and the proposed method are overall clearly presented . the current recommendation is weak reject because the experimental study is not convincing or comprehensive enough . 1 .although the goal of this work is to deal with the inefficiency issue of the objective function of existing dpn with the newly proposed one  this experimental study does not seem to conduct sufficient experiments to demonstrate the advantages  say  in terms of training efficiency  the capability in making the network scalable for more challenging dataset  of the proposed objective function over the existing one  2 . table 1 compares the proposed method with odin . however  as indicated in this work  odin is trained with indomain examples only . is this comparison fair  actually  odin s setting seems to be more practical and more challenging than the setting used by the propose methods . 3 .the evaluation criteria shall be better explained at the beginning of the experiment  especially how they can be collectively used to verify that the proposed method can better distinguish distributional uncertainty from other uncertainty types . 4 .in addition  the experimental study can be clearer on the training and test splits . [DEPT] how many samples from cifar10 and cifar100 are used for training and test purpose  respectively  also  since training examples are from cifar10 and cifar100 and the test examples are also from these two datasets  does this contradict with the motivation of distributional mismatch between training and test examples mentioned in the abstract  5 .the experimental study can have more comparison on challenging datasets with more classes since it is indicated that dpn has difficulty in dealing with a large number of classes . minor  1 . please define the hattheta in eq .also  is the dirac delta estimation a good enough approximation here  2 .the lambda  out   lambda  in  in eq .  11  needs to be better explained . in particular  are the first terms in eq .  10  and eq .  11  comparable in terms of magnitude  otherwise  lambda  out   lambda  in  may not make sense . 3 .the novelty and significance of finetuning the proposed model with noisy ood training images can be better justified .\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m \u001b[0mmasker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaskers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"transformers tokenizer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0mexplainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_prediction_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0mshap_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'shap' has no attribute 'maskers'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import transformers\n",
    "import datasets\n",
    "import shap\n",
    "\n",
    "labels = classes = ['MOT', 'CLA', 'SOU','SUB', 'MEA', 'ORI', 'REP']\n",
    "\n",
    "'''\n",
    "method = \"custom tokenizer\"\n",
    "\n",
    "# build an explainer by passing a transformers tokenizer\n",
    "if method == \"transformers tokenizer\":\n",
    "    explainer = shap.Explainer(f, tokenizer, output_names=labels)\n",
    "\n",
    "# build an explainer by explicitly creating a masker\n",
    "elif method == \"default masker\":\n",
    "    masker = shap.maskers.Text(r\"\\W\") # this will create a basic whitespace tokenizer\n",
    "    explainer = shap.Explainer(f, tokenizer, output_names=labels)\n",
    "\n",
    "# build a fully custom tokenizer\n",
    "elif method == \"custom tokenizer\":\n",
    "    import re\n",
    "    \n",
    "    def custom_tokenizer(s, return_offsets_mapping=True):\n",
    "        \"\"\" Custom tokenizers conform to a subset of the transformers API.\n",
    "        \"\"\"\n",
    "        pos = 0\n",
    "        offset_ranges = []\n",
    "        input_ids = []\n",
    "        for m in re.finditer(r\"\\W\", s):\n",
    "            start, end = m.span(0)\n",
    "            offset_ranges.append((pos, start))\n",
    "            input_ids.append(s[pos:start])\n",
    "            pos = end\n",
    "        if pos != len(s):\n",
    "            offset_ranges.append((pos, len(s)))\n",
    "            input_ids.append(s[pos:])\n",
    "        out = {}\n",
    "        out[\"input_ids\"] = input_ids\n",
    "        if return_offsets_mapping:\n",
    "            out[\"offset_mapping\"] = offset_ranges\n",
    "        print(out)\n",
    "        return out\n",
    "\n",
    "    masker = shap.maskers.Text(custom_tokenizer)\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "#If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda:1\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "  print('No GPU available, using the CPU instead.')\n",
    "  device = torch.device(\"cpu\")\n",
    "\n",
    "#torch.cuda.set_device(0)\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import pickle\n",
    "import torch\n",
    "from tqdm.autonotebook import tqdm\n",
    "from transformers import AutoTokenizer,AutoModel\n",
    "\n",
    "model_name='allenai/scibert_scivocab_uncased'\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "'''\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Data(Dataset):\n",
    "    def __init__(self,embed,aspects,sentiments):\n",
    "\n",
    "        self.labels=aspects\n",
    "        self.labels_s=sentiments\n",
    "        self.sentences=embed\n",
    "        self.max_len=50\n",
    "        self.size=len(embed)\n",
    "\n",
    "    @classmethod\n",
    "    def getReader(cls,low,up,test=None,r=1):\n",
    "        if(True):\n",
    "          with open(\"data/onehot_aspect_multitask.pkl\",'rb') as out:\n",
    "              labels =pickle.load(out)\n",
    "              labels = labels[low:up]\n",
    "          \n",
    "          with open(\"data/onehot_sentiment_multitask.pkl\",'rb') as out:\n",
    "              labels_s=pickle.load(out)\n",
    "              labels_s = labels_s[low:up]\n",
    "          \n",
    "          with open(\"data/dataframe_multitask.pkl\",'rb') as out:\n",
    "              data_s=pickle.load(out)\n",
    "              sents = list(data_s['sentences'])[low:up]\n",
    "        \n",
    "        assert len(labels) == len(sents) ==len(labels_s)\n",
    "        print(\"Total number of Reviews\", len(labels))\n",
    "        \n",
    "        return cls(sents, labels,labels_s)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "\n",
    "        sen=self.sentences[idx]\n",
    "\n",
    "        inputs = tokenizer(sen,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            return_length = True,\n",
    "            truncation=True)\n",
    "        \n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs['token_type_ids']\n",
    "        \n",
    "        return {\n",
    "            'ids_sen': torch.tensor(ids),\n",
    "            'mask_sen': torch.tensor(mask),\n",
    "            'token_type_ids_sen': torch.tensor(token_type_ids, dtype=torch.float64),\n",
    "            'targets':self.labels[idx],\n",
    "            'targets_senti':self.labels_s[idx]}\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "def getLoaders (batch_size):\n",
    "\n",
    "        print('Reading the training Dataset...')\n",
    "        print()\n",
    "        train_dataset = Data.getReader(0,100000) #19200 #21216\n",
    "        \n",
    "        print()\n",
    "\n",
    "        print('Reading the validation Dataset...')\n",
    "        print()\n",
    "        valid_dataset = Data.getReader(100000, 148000) #23200 #25216\n",
    "\n",
    "        print('Reading the test Dataset...')\n",
    "        print()\n",
    "        test_dataset = Data.getReader(148000, 218000) #23200:25248\n",
    "        \n",
    "        trainloader = DataLoader(dataset=train_dataset, batch_size = batch_size, num_workers=8,shuffle=True)\n",
    "        validloader = DataLoader(dataset=valid_dataset, batch_size = batch_size, num_workers=8,shuffle=True)\n",
    "        testloader = DataLoader(dataset=test_dataset, batch_size = batch_size, num_workers=8)\n",
    "        \n",
    "        return trainloader, validloader, testloader\n",
    "\n",
    "trainloader, validloader, testloader = getLoaders(batch_size)\n",
    "\n",
    "print(\"Length of TrainLoader:\",len(trainloader))\n",
    "print(\"Length of ValidLoader:\",len(validloader))\n",
    "print(\"Length of TestLoader:\",len(testloader))\n",
    "'''\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class BertModel(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "\n",
    "        super(BertModel, self).__init__()\n",
    "        self.model=AutoModel.from_pretrained(model_name,  output_hidden_states=True)\n",
    "\n",
    "        #for param in self.model.parameters():\n",
    "        #param.require_grad=True\n",
    "          \n",
    "        self.in_features = in_features   #768\n",
    "        self.out_features = out_features    #7\n",
    "\n",
    "        self.flatten=nn.Flatten()\n",
    "        self.lstm_1 = nn.LSTM(in_features, 200//2, batch_first=True, bidirectional=True) #bidirectional=True\n",
    "    \n",
    "      \n",
    "        self.linear1=nn.Linear(200*out_features,200*2)\n",
    "        self.linear2=nn.Linear(200*2,256)\n",
    "        self.linear3=nn.Linear(256,64)\n",
    "\n",
    "        self.linear1_sen=nn.Linear(200,64)\n",
    "        self.linear2_sen=nn.Linear(64,2)\n",
    "   \n",
    "        self.last_dense = nn.Linear(64, self.out_features)\n",
    "        self.dropout1=nn.Dropout(p=0.5)\n",
    "        self.dropout2=nn.Dropout(p=0.2)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid=nn.Sigmoid()\n",
    "        self.tanh=nn.Tanh()\n",
    "\n",
    "        category = torch.rand(200, out_features,requires_grad=True)  #(512,7)\n",
    "        nn.init.xavier_normal_(category)\n",
    "\n",
    "        self.category=category.to(device)\n",
    "        #self.category_sent=category_sent.to(device)\n",
    "\n",
    "    def forward(self, t1,strategy:str):\n",
    "        \n",
    "        ids, mask, token_type_ids = t1\n",
    "        encoded_layers = self.model(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict=False)[2]\n",
    "        scibert_hidden_layer = encoded_layers\n",
    "        \n",
    "        if(strategy=='last_4'):\n",
    "          scibert_hidden_layers=torch.cat((scibert_hidden_layer[-1],\n",
    "                                        scibert_hidden_layer[-2],\n",
    "                                        scibert_hidden_layer[-3],\n",
    "                                        scibert_hidden_layer[-4]),dim=2)\n",
    "          \n",
    "        if(strategy=='last'):\n",
    "          scibert_hidden_layers=encoded_layers[12]\n",
    "\n",
    "\n",
    "        if(strategy=='mean'):\n",
    "          scibert_hidden_layers=torch.mean(encoded_layers,dim=2)\n",
    "      \n",
    "\n",
    "        s_e=scibert_hidden_layers                  #(32,13,768)\n",
    "\n",
    "        h0 = torch.zeros(2, s_e.size(0), 200 // 2)\n",
    "        c0 = torch.zeros(2, s_e.size(0), 200 // 2)\n",
    "        h0, c0 = h0.to(device), c0.to(device)\n",
    "        s_e, (hn, cn) = self.lstm_1(s_e, (h0, c0))    #(32,13,512)\n",
    "      \n",
    "  \n",
    "        c=self.category.unsqueeze(0)            #(1,512,7)\n",
    "        comp = torch.matmul(s_e,c)              #(32,13,7)\n",
    "        comp = comp.permute(0,2,1)             #(32,7,13)\n",
    "\n",
    "        comp1=    self.relu(self.linear1_sen(s_e))         #(32,50,256)\n",
    "        comp1 =   self.linear2_sen(comp1)                  #(32,50,2)\n",
    "\n",
    "  \n",
    "        wts = F.softmax(comp, dim=2) #(32,7,13)\n",
    "        wts1= torch.bmm(wts,comp1)   #(32,7,2)\n",
    "        wts_temp=wts1\n",
    "        \n",
    "        e=torch.bmm(wts,s_e)       #(32,7,512)\n",
    "\n",
    "        l = torch.reshape(e, (ids.size(0), 200*7))\n",
    "\n",
    "        l = self.relu(self.linear1(l))\n",
    "        l = self.dropout1(l)\n",
    "        l = self.relu(self.linear2(l))\n",
    "        l = self.dropout1(l)\n",
    "        l = self.relu(self.linear3(l))\n",
    "\n",
    "        model_output = self.sigmoid(self.last_dense(l))\n",
    "        model_output_sent = self.sigmoid(wts1)\n",
    "        \n",
    "        del l,comp,s_e,hn,cn,scibert_hidden_layer,ids,mask,token_type_ids\n",
    "      \n",
    "        return model_output, wts,comp1,model_output_sent\n",
    "\n",
    "\n",
    "text_model = BertModel(768,7)\n",
    "text_model.to(device)\n",
    "\n",
    "'''\n",
    "criterion1 = nn.BCELoss()\n",
    "criterion2 = nn.BCELoss()\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in text_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in text_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.01}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=1000)\n",
    "\n",
    "\n",
    "text_model.train()\n",
    "result=[]\n",
    "EPOCH=10\n",
    "\n",
    "train_out = []\n",
    "val_out = []\n",
    "train_true = []\n",
    "val_true = []\n",
    "test_out = []\n",
    "test_true = []\n",
    "attn_train = []\n",
    "attn_val = []\n",
    "attn_test = []\n",
    "attn_test_senti=[]\n",
    "test_out_senti=[]\n",
    "test_true_senti=[]\n",
    "loss_log1 = []\n",
    "loss_log2 = []\n",
    "\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "\n",
    "  final_train_loss=0.0\n",
    "  final_val_loss=0.0\n",
    "  l1 = []\n",
    "  text_model.train()\n",
    "\n",
    "  for idx,data in tqdm(enumerate(trainloader),desc=\"Train epoch {}/{}\".format(epoch + 1, EPOCH)):\n",
    "\n",
    "    ids = data['ids_sen'].to(device,dtype = torch.long)\n",
    "    mask = data['mask_sen'].to(device,dtype = torch.long)\n",
    "    token_type_ids = data['token_type_ids_sen'].to(device,dtype = torch.long)\n",
    "    targets = data['targets'].to(device,dtype = torch.float)\n",
    "    targets_s = data['targets_senti'].to(device,dtype = torch.float)\n",
    "    \n",
    "    t1 = (ids,mask,token_type_ids)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    out, attn_t,_,out_sen = text_model(t1,'last')\n",
    "\n",
    "    if (epoch+1 == EPOCH):\n",
    "      train_out.append((torch.transpose(out,0,1)).detach().cpu())\n",
    "      train_true.append((torch.transpose(targets,0,1)).detach().cpu())\n",
    "\n",
    "    loss = (criterion1(out, targets)+criterion2(out_sen, targets_s))/2\n",
    "    l1.append(loss.item())\n",
    "    final_train_loss +=loss.item()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if idx % 100 == 0:\n",
    "      scheduler.step()\n",
    "\n",
    "    \n",
    "  loss_log1.append(np.average(l1))\n",
    "\n",
    "  text_model.eval()\n",
    "  l2 = []\n",
    "\n",
    "  \n",
    "  for data in tqdm(validloader,desc=\"Valid epoch {}/{}\".format(epoch + 1, EPOCH)):\n",
    "    ids = data['ids_sen'].to(device,dtype = torch.long)\n",
    "    mask = data['mask_sen'].to(device,dtype = torch.long)\n",
    "    token_type_ids = data['token_type_ids_sen'].to(device,dtype = torch.long)\n",
    "    targets = data['targets'].to(device,dtype = torch.float)\n",
    "    targets_s = data['targets_senti'].to(device,dtype = torch.float)\n",
    "    \n",
    "    t1 = (ids,mask,token_type_ids)\n",
    "    \n",
    "    out_val, attn_v ,_,out_val_senti= text_model(t1,'last')\n",
    "\n",
    "    if (epoch+1 == EPOCH):\n",
    "      val_out.append((torch.transpose(out_val,0,1)).detach().cpu())\n",
    "      val_true.append((torch.transpose(targets,0,1)).detach().cpu())\n",
    "\n",
    "    loss = (criterion1(out_val, targets) + criterion2(out_val_senti, targets_s))/2\n",
    "    l2.append(loss.item())\n",
    "    final_val_loss+=loss.item()\n",
    "\n",
    "  loss_log2.append(np.average(l2))\n",
    "  curr_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "  print(\"Epoch {}, loss: {}, val_loss: {}\".format(epoch+1, final_train_loss/len(trainloader), final_val_loss/len(validloader)))\n",
    "  print()\n",
    "  \n",
    "'''\n",
    "\n",
    "def f(x):\n",
    "\n",
    "  # print(encoded)\n",
    "  inputs = tokenizer(x,add_special_tokens=False,\n",
    "    return_token_type_ids=True,\n",
    "    return_length = True,\n",
    "    truncation=False)\n",
    "  # #inputs = torch.tensor([tokenizer.encode(v, padding='max_length', max_length=128, truncation=True) for v in x]).cuda()\n",
    "        \n",
    "  ids = torch.tensor(inputs['input_ids']).to(device,dtype = torch.long)\n",
    "  mask = torch.tensor(inputs['attention_mask']).to(device,dtype = torch.long)\n",
    "  token_type_ids = torch.tensor(inputs['token_type_ids']).to(device,dtype = torch.long)\n",
    "  t1=(ids,mask,token_type_ids)\n",
    "  \n",
    "  out_test, attn_T,attn_T_S,out_test_senti = text_model(t1,'last')\n",
    "  ret = out_test.cpu().detach().numpy()\n",
    "  print(ret)\n",
    "  return ret\n",
    "\n",
    "\n",
    "def model_prediction_gpu(x):\n",
    "    tv = torch.tensor([tokenizer.encode(v, padding='max_length', \n",
    "                                        max_length=512, truncation=True) for v in x]).to(device,dtype = torch.long)\n",
    "    attention_mask = (tv!=0).type(torch.int64).to(device,dtype = torch.long)\n",
    "    token_type_ids = torch.zeros_like(attention_mask).to(device,dtype = torch.long)\n",
    "    t1=(tv,attention_mask,token_type_ids)\n",
    "    out_test, attn_T,attn_T_S,out_test_senti = text_model(t1,'last')\n",
    "    val = torch.logit(out_test).detach().cpu().numpy()\n",
    "    return val\n",
    "\n",
    "\n",
    "method = \"custom tokenizer\"\n",
    "\n",
    "# build an explainer by passing a transformers tokenizer\n",
    "if method == \"transformers tokenizer\":\n",
    "    explainer = shap.Explainer(f, tokenizer, output_names=labels)\n",
    "\n",
    "# build an explainer by explicitly creating a masker\n",
    "elif method == \"default masker\":\n",
    "    masker = shap.maskers.Text(r\"\\W\") # this will create a basic whitespace tokenizer\n",
    "    explainer = shap.Explainer(f, masker, output_names=labels)\n",
    "\n",
    "# build a fully custom tokenizer\n",
    "elif method == \"custom tokenizer\":\n",
    "    import re\n",
    "\n",
    "    def custom_tokenizer(s, return_offsets_mapping=True):\n",
    "        \"\"\" Custom tokenizers conform to a subset of the transformers API.\n",
    "        \"\"\"\n",
    "        pos = 0\n",
    "        offset_ranges = []\n",
    "        input_ids = []\n",
    "        for m in re.finditer(r\"\\W\", s):\n",
    "            start, end = m.span(0)\n",
    "            offset_ranges.append((pos, start))\n",
    "            input_ids.append(s[pos:start])\n",
    "            pos = end\n",
    "        if pos != len(s):\n",
    "            offset_ranges.append((pos, len(s)))\n",
    "            input_ids.append(s[pos:])\n",
    "        out = {}\n",
    "        out[\"input_ids\"] = input_ids\n",
    "        if return_offsets_mapping:\n",
    "            out[\"offset_mapping\"] = offset_ranges\n",
    "        return out\n",
    "\n",
    "\n",
    "# shap_values = explainer(list(data['text'][:3]))\n",
    "x = [\"hi\",\"hello\"]\n",
    "\n",
    "masker = shap.maskers.Text(\"transformers tokenizer\")\n",
    "explainer = shap.Explainer(model_prediction_gpu, tokenizer, output_names=labels)\n",
    "print(f(['[MASK]']))\n",
    "shap_values = explainer(x)\n",
    "\n",
    "shap.plots.text(shap_values)\n",
    "\n",
    "'''\n",
    "with torch.no_grad():\n",
    "   for data in testloader:\n",
    "     ids = data['ids_sen'].to(device,dtype = torch.long)\n",
    "     mask = data['mask_sen'].to(device,dtype = torch.long)\n",
    "     token_type_ids = data['token_type_ids_sen'].to(device,dtype = torch.long)\n",
    "     targets = data['targets'].to(device,dtype = torch.float)\n",
    "     targets_s = data['targets_senti'].to(device,dtype = torch.float)\n",
    "\n",
    "     t1=(ids,mask,token_type_ids)\n",
    "  \n",
    "     out_test, attn_T,attn_T_S,out_test_senti = text_model(t1,'last')\n",
    "\n",
    "     test_out.append((torch.transpose(out_test,0,1)).detach().cpu())\n",
    "     test_true.append((torch.transpose(targets,0,1)).detach().cpu())\n",
    "     test_true_senti.append((torch.transpose(targets_s,0,1)).detach().cpu())\n",
    "     attn_test.append((torch.tensor(attn_T)).detach().cpu())\n",
    "     attn_test_senti.append((torch.tensor(attn_T_S)).detach().cpu())\n",
    "     test_out_senti.append((torch.transpose(out_test_senti,0,1)).detach().cpu())\n",
    "     \n",
    "\n",
    "plt.plot(range(len(loss_log1)), loss_log1)\n",
    "plt.plot(range(len(loss_log2)), loss_log2)\n",
    "plt.savefig('graphs/loss_bert_multi.png')\n",
    "\n",
    "torch.save(text_model.state_dict(), \"ckpt/stsb-robert7_10_multi_full_data.pt\")\n",
    "\n",
    "train_out = torch.cat(train_out, 1)\n",
    "val_out = torch.cat(val_out, 1)\n",
    "train_true = torch.cat(train_true, 1)\n",
    "val_true = torch.cat(val_true, 1)\n",
    "test_out = torch.cat(test_out, 1)\n",
    "test_out_senti = torch.cat(test_out_senti, 1)\n",
    "test_true = torch.cat(test_true, 1)\n",
    "attn_test = torch.cat(attn_test, 0)\n",
    "attn_test_senti = torch.cat(attn_test_senti, 0)\n",
    "test_true_senti=torch.cat(test_true_senti, 1)\n",
    "\n",
    "#train_out, val_out, train_true, val_true = train_out.cpu(), val_out.cpu(), train_true.cpu(), val_true.cpu()\n",
    "#test_out, test_true = test_out.cpu(), test_true.cpu()\n",
    "#attn_test = attn_test.cpu()\n",
    "\n",
    "attnfile = open('outputs/attn_noaspect_multi.pkl', 'wb')\n",
    "pickle.dump(attn_test, attnfile)\n",
    "\n",
    "attnfile_ss = open('outputs/attn_sentiment_multi.pkl', 'wb')\n",
    "pickle.dump(attn_test_senti, attnfile_ss)\n",
    "\n",
    "test_out_ = (test_out, test_true)\n",
    "test_out_senti_=(test_out_senti,test_true_senti)\n",
    "\n",
    "test_outs = open('outputs/main_bert7_test_out_noaspect_multi.pkl', 'wb')\n",
    "pickle.dump(test_out_, test_outs)\n",
    "\n",
    "test_outs_sentiment = open('outputs/main_bert7_test_out_sentiments_multi_good.pkl', 'wb')\n",
    "pickle.dump(test_out_senti_, test_outs_sentiment)\n",
    "\n",
    "\n",
    "\n",
    "f=open(\"results/\"+\"stsb-robert_7_multi\"+\".txt\",'w')\n",
    "f.close()\n",
    "\n",
    "def labelwise_metrics(pred, true, split):\n",
    "  f=open(\"results/\"+\"stsb-robert_7_multi\"+\".txt\",'a')\n",
    "  f.write('-'*25 + split + '-'*25 + '\\n\\n')\n",
    "   \n",
    "  pred = (pred>0.425)\n",
    "\n",
    "  #batch_size = len(pred)\n",
    "  batch_size = 16\n",
    "\n",
    "  pred = pred.to(torch.int)\n",
    "  true = true.to(torch.int)\n",
    "\n",
    "  from sklearn.metrics import accuracy_score\n",
    "  from sklearn.metrics import confusion_matrix\n",
    "\n",
    "  for i in range(batch_size):\n",
    "    acc=accuracy_score(true[i],pred[i])\n",
    "\n",
    "    epsilon = 1e-7\n",
    "    confusion_vector = pred[i]/true[i]\n",
    "\n",
    "    true_positives = torch.sum(confusion_vector == 1).item()\n",
    "    false_positives = torch.sum(confusion_vector == float('inf')).item()\n",
    "    true_negatives = torch.sum(torch.isnan(confusion_vector)).item()\n",
    "    false_negatives = torch.sum(confusion_vector == 0).item()\n",
    "\n",
    "    precision = true_positives/(true_positives+false_positives+epsilon)\n",
    "    recall = true_positives/(true_positives+false_negatives+epsilon)\n",
    "    f1 = 2*precision*recall/(precision+recall+epsilon)\n",
    "\n",
    "    print(\"Label: {}, acc: {:.3f}, f1: {:.3f}\".format(i+1, acc, f1))\n",
    "    f.write(\"Label: {}, acc: {:.3f}, f1: {:.3f}\\n\".format(i+1, acc, f1))\n",
    "    f.write(str(confusion_matrix(true[i], pred[i])))\n",
    "    f.write('\\n')\n",
    "\n",
    "  return 0\n",
    "\n",
    "f1=open(\"results/\"+\"stsb-robert_7_multi_senti\"+\".txt\",'w')\n",
    "f1.close()\n",
    "\n",
    "def labelwise_metrics_senti(pred, true, split):\n",
    "\n",
    "  classes = ['MOT +', 'MOT -', 'CLA +', 'CLA -', 'SOU +', 'SOU -', 'SUB +', 'SUB -', 'MEA +', 'MEA -', 'ORI +', 'ORI -', 'REP +', 'REP -']\n",
    "\n",
    "  f=open(\"results/\"+\"stsb-robert_7_multi_senti\"+\".txt\",'a')\n",
    "  f.write('-'*25 + split + '-'*25 + '\\n\\n')\n",
    "   \n",
    "  pred = (pred>0.425)\n",
    "\n",
    "  pred = pred.to(torch.int)\n",
    "  true = true.to(torch.int)\n",
    "  pred = pred.reshape(7, 2, -1)\n",
    "  true = true.reshape(7, 2, -1)\n",
    "  pred = pred.reshape(14, -1)\n",
    "  true = true.reshape(14, -1)\n",
    "\n",
    "  batch_size = len(pred)\n",
    "\n",
    "  from sklearn.metrics import accuracy_score\n",
    "  from sklearn.metrics import confusion_matrix\n",
    "\n",
    "  for i in range(batch_size):\n",
    "    acc=accuracy_score(true[i],pred[i])\n",
    "\n",
    "    epsilon = 1e-7\n",
    "    confusion_vector = pred[i]/true[i]\n",
    "\n",
    "    true_positives = torch.sum(confusion_vector == 1).item()\n",
    "    false_positives = torch.sum(confusion_vector == float('inf')).item()\n",
    "    true_negatives = torch.sum(torch.isnan(confusion_vector)).item()\n",
    "    false_negatives = torch.sum(confusion_vector == 0).item()\n",
    "\n",
    "    precision = true_positives/(true_positives+false_positives+epsilon)\n",
    "    recall = true_positives/(true_positives+false_negatives+epsilon)\n",
    "    f1 = 2*precision*recall/(precision+recall+epsilon)\n",
    "\n",
    "    #print(\"Class: {}, acc: {:.3f}, f1: {:.3f}\".format(classes[i], acc, f1))\n",
    "    f.write(\"Label: {}, acc: {:.3f}, f1: {:.3f}\\n\".format(classes[i], acc, f1))\n",
    "    f.write(str(confusion_matrix(true[i], pred[i])))\n",
    "    f.write('\\n')\n",
    "\n",
    "  return 0\n",
    "\n",
    "\n",
    "print('Training...')\n",
    "labelwise_metrics(train_out, train_true, 'TRAINING')\n",
    "print()\n",
    "print('Validation...')\n",
    "labelwise_metrics(val_out, val_true, 'VALIDATION')\n",
    "print()\n",
    "print('Test...')\n",
    "labelwise_metrics(test_out, test_true, 'TESTING')\n",
    "print('Training...')\n",
    "labelwise_metrics_senti(train_out, train_true, 'TRAINING')\n",
    "print()\n",
    "print('Validation...')\n",
    "labelwise_metrics_senti(val_out, val_true, 'VALIDATION')\n",
    "print()\n",
    "print('Test...')\n",
    "labelwise_metrics_senti(test_out_senti, test_true_senti, 'TESTING')\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hardik",
   "language": "python",
   "name": "hardik"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
